# -*- coding: utf-8 -*-
"""cancer detection from gene expression level.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19mKTFeu4flA49KT-qMlAWlU56_F6hydy

# Import Python Libraries
"""

#data handling
import pandas as pd
import numpy as np

#data visualization
import matplotlib.pyplot as plt
import seaborn as sns

#preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler

#classification
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense

#detect the percentage of each cancer
from sklearn.linear_model import LogisticRegression

"""# Read Data"""

# first upload the needed data in this notebook's folder

df=pd.read_csv("/content/Cancer_Data_for_NN/cancer_gene_expression.csv")

"""# Data Exploration & Cleaning"""

# check how many rows and coloumns are there

print(df.shape)

#check only first five rows with all coloumn to guess the data

df.head()

# check the name of the last column of this dataframe
# and to view that coloumn's data use "print" function

df.columns[-1]

# check the missing values

datanul = df.isnull().sum()
g=[i for i in datanul if i>0]

print('columns with missing values:%d'%len(g))

#let's check how many different cancer types are there in the data

print(df['Cancer_Type'].value_counts())

"""# Data Preprocessing

This is done to put the data in an appropriate format before modelling. #we will now seperate the feature values from the class. we do this because scikit-learn requires that features and class are separated before parsing them to the classifiers.
"""

X= df.iloc[:,0:-1]
y= df.iloc[:,-1]

#let's encode target labels (y) with values between 0 and n_classes-1.

label_encoder=LabelEncoder()
label_encoder.fit(y)
y=label_encoder.transform(y)
labels=label_encoder.classes_
classes=np.unique(y)
nclasses=np.unique(y).shape[0]

#split data into training,validation and test sets

#split the data into training and test sets
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)

#split the training set into two (training and validation)
X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.2)

# Scale the data between 0-1
min_max_scaler=MinMaxScaler()
X_train=min_max_scaler.fit_transform(X_train)
X_val=min_max_scaler.transform(X_val)
X_test=min_max_scaler.transform(X_test)

"""# Build the Neural Network Model"""

#define model
model = Sequential()

#hidden layer 1
model.add(Dense(40, input_dim=X_train.shape[1], activation='relu'))

#hidden layer 2
model.add(Dense(20, activation='relu'))

#output layer
model.add(Dense(nclasses, activation='softmax'))

#define optimizer and learning rate
opt_adam = keras.optimizers.Adam(learning_rate=0.001)

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=opt_adam, metrics=[keras.metrics.SparseCategoricalAccuracy()])

#fit the model to the training data

history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32,epochs=200, verbose=1)

predictions = model.predict(X_test)

accuracy = model.evaluate(X_test, y_test, verbose=0)

#get the predictions for the first 20 samples in the test set

for index,entry in enumerate(predictions[0:10,:]):
    print('predicted:%d ,actual:%d'%(np.argmax(entry),y_test[index]))

# summarize history for accuracy

plt.plot(history.history['sparse_categorical_accuracy'])
plt.plot(history.history['val_sparse_categorical_accuracy'])
plt.title('model performance')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='lower right')
plt.show()

# summarize history for loss

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='lower right')
plt.show()

# Train a machine learning model to classify cancer cases
model = LogisticRegression()
model.fit(X, y)

# Use the model to predict the probability of cancer for each case
probabilities = model.predict_proba(X)

# Compute the average probability of cancer for all cases
average_probability = np.mean(probabilities[:, 1])

# Compute the cancer percentage based on the average probability
cancer_percentage = average_probability * 100

print(f"Cancer percentage: {cancer_percentage:.2f}%")

# Split the dataset into features, labels, and cancer type
X = df.drop(['gene_4', 'Cancer_Type'], axis=1)
y = df['gene_4']
cancer_type = df['Cancer_Type']

# Ensure that X and X_train have the same number of features
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
assert X_train.shape[1] == X.shape[1]

# Remove feature names from X
X = X.values
X_train = X_train.values
X_val = X_val.values

# Convert y to a categorical variable using LabelEncoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Train a machine learning model to classify cancer cases for each cancer type
model = LogisticRegression()
model.fit(X_train, y_train)

# Use the model to predict the probability of cancer for each case
probabilities = model.predict_proba(X)

# Compute the average probability of cancer for each cancer type
average_probabilities = np.mean(probabilities, axis=0)

# Compute the cancer percentage for each cancer type
cancer_percentages = average_probabilities * 100

# Print the cancer percentage for each cancer type
for i, cancer in enumerate(cancer_type.unique()):
    print(f"Cancer type: {cancer}, Cancer percentage: {cancer_percentages[i]:.2f}%")